{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model 256 C1 vs C2-3 vs all_other.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9725RbwsB_ja"
      },
      "source": [
        "###  <span style=\"color:red\">**This Notebook can be run from Google Colab:**</span>\n",
        "\n",
        "https://colab.research.google.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ks-1Im7ECAim",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Dense, Activation, Dropout, BatchNormalization,\\\n",
        "                          Conv2D, MaxPooling2D, Flatten, AveragePooling2D,\\\n",
        "                          GlobalAveragePooling2D, ZeroPadding2D\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import RMSprop, Adam, Adamax, Nadam, SGD\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, \\\n",
        "                            classification_report\n",
        "\n",
        "# Import PyDrive and associated libraries (to connect with GoogleDrive)\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# disable warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DItexGaqdfEA"
      },
      "source": [
        "### **Check if we are using GPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vn7AYx74dNq6",
        "outputId": "8169dd15-4a4c-47d5-e686-87ea2bea2eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "if K.backend() == \"tensorflow\":\n",
        "    import tensorflow as tf\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name == '':\n",
        "        device_name = \"None\"\n",
        "    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow version: 1.15.0 , GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QQXfhMrHlqrz"
      },
      "source": [
        "### **Download Patches from GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ACvUlEucnnY0",
        "outputId": "847b4513-2748-46d5-828c-935b7d853047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1GrvQWYf90c9lE-qPCovkZA8Ai0cayzkP' #NEW 128x128_s60_no border_minpospix_1250 minposval_1024 ROTATE_EVERY_45\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])\n",
        "print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"Img_Patches_256.zip\"\n",
            "Root dir content: ['.config', 'Img_Patches_256.zip', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSdGQav-qEJM"
      },
      "source": [
        "### **Unzip the Patches:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jP1-2THkn47w",
        "outputId": "3b1fb866-a2cc-42af-e91c-d1d7ca5bbf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove 'Patches' dir if it already exists\n",
        "# if 'Patches' in os.listdir():\n",
        "#   shutil.rmtree('./Patches')\n",
        "with zipfile.ZipFile(downloaded['title'],\"r\") as zip:\n",
        "    zip.extractall()\n",
        "os.remove(downloaded['title'])\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root dir content: ['.config', 'content', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8a8Y1DhKL5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43ff7471-2b15-44d0-f250-cca8ab3d416e"
      },
      "source": [
        ""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nd7htfD3rJ6V"
      },
      "source": [
        "### **Let's count patches by type and class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DjJy7FFsvm5H",
        "outputId": "89dabe90-388e-42ba-bb25-baa9726d11f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['1','2','3','4','5','6','7','0']\n",
        "\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    # class_weights[type]['pos'] = {} # nested dictionary to store class weights\n",
        "    # class_weights[type]['neg'] = {} # nested dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pfolder = './Img_Patches/{}/{}'.format(type,cls)\n",
        "        n = len(os.listdir(pfolder))\n",
        "        # total = n_pos + n_neg\n",
        "        n_type += n\n",
        "        # type_pos += n_pos\n",
        "        # type_neg += n_neg\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,n,n,0))\n",
        "        class_weights[type]['{}'.format(cls)] = 1/n if n else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,n_type,0))\n",
        "    for loc in class_weights[type].keys():\n",
        "        class_weights[type][loc] *= n_type\n",
        "    grand_total += n_type\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,grand_total,0))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 162 = 162 positive + 0 negative\n",
            "total_3: 614 = 614 positive + 0 negative\n",
            "total_4: 303 = 303 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_6: 169 = 169 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 418 = 418 positive + 0 negative\n",
            "Total Serial: 1883 = 1883 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_1: 13 = 13 positive + 0 negative\n",
            "total_2: 20 = 20 positive + 0 negative\n",
            "total_3: 77 = 77 positive + 0 negative\n",
            "total_4: 40 = 40 positive + 0 negative\n",
            "total_5: 10 = 10 positive + 0 negative\n",
            "total_6: 19 = 19 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 106 = 106 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_1: 305 = 305 positive + 0 negative\n",
            "total_2: 431 = 431 positive + 0 negative\n",
            "total_3: 880 = 880 positive + 0 negative\n",
            "total_4: 522 = 522 positive + 0 negative\n",
            "total_5: 279 = 279 positive + 0 negative\n",
            "total_6: 378 = 378 positive + 0 negative\n",
            "total_7: 56 = 56 positive + 0 negative\n",
            "total_0: 478 = 478 positive + 0 negative\n",
            "Total Streak: 3329 = 3329 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 5500 = 5500 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TLfSmpKFPLFu"
      },
      "source": [
        "### **Let's downsample training ('Serial') classes to no more than the minimum between 'C1' and 'C2-3':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPt4zNrWPJed",
        "outputId": "b6668f6e-be8b-4096-e47c-b8367e1d36cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pfolder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n = len(os.listdir(pfolder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))\n",
        "\n",
        "if totals['1'] < totals['2']:\n",
        "    minority = '1'\n",
        "else:\n",
        "    minority = '2'\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    #if key != minority and key not in ['C1','C2-3']:\n",
        "    if key != minority:\n",
        "        n_to_delete = max(value-n_min, 0)\n",
        "        root = './Img_Patches/{}/{}/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pfolder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n = len(os.listdir(pfolder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 162 = 162 positive + 0 negative\n",
            "total_3: 614 = 614 positive + 0 negative\n",
            "total_4: 303 = 303 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_6: 169 = 169 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 418 = 418 positive + 0 negative\n",
            "Total Serial: 1883 = 1883 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 106 = 106 positive + 0 negative\n",
            "total_3: 106 = 106 positive + 0 negative\n",
            "total_4: 106 = 106 positive + 0 negative\n",
            "total_5: 106 = 106 positive + 0 negative\n",
            "total_6: 106 = 106 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 106 = 106 positive + 0 negative\n",
            "Total Serial: 745 = 745 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e8a8ZcfKKvw5"
      },
      "source": [
        "### **Let's move all patches for classes other than C1 or C2-3, to a single folder called 'all_other':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dz9V2uuxLM50",
        "colab": {}
      },
      "source": [
        "# for patch_type in ['Positive','Negative']:\n",
        "#     if patch_type == 'Positive': sufix = '_pos'\n",
        "#     if patch_type == 'Negative': sufix = '_neg'\n",
        "for type_ in ['Serial', 'Control', 'Streak']:\n",
        "    folder = './Img_Patches/{}/'.format(type_)\n",
        "    if 'all_other' not in os.listdir(folder):\n",
        "        dest = folder + 'all_other'\n",
        "        os.mkdir(dest)\n",
        "    main_folder = '/content/content'\n",
        "    for cls in classes:\n",
        "        if cls in ['1','2']: continue\n",
        "        cls_folder = folder + cls\n",
        "        for patch in os.listdir(cls_folder):\n",
        "            source = cls_folder + '/' + patch\n",
        "            shutil.copy(os.path.join(main_folder, source), os.path.join(main_folder,dest))\n",
        "            # shutil.move(source, dest)\n",
        "        shutil.rmtree(cls_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1sPAFxK7REM7"
      },
      "source": [
        "### **Let's count patches by type and class again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmKlpYPZRE-_",
        "outputId": "4a5fae70-c24d-4fdd-f566-9ef93ff92da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['1','2','all_other']\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    # class_weights[type]['pos'] = {} # nested dictionary to store class weights\n",
        "    # class_weights[type]['neg'] = {} # nested dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pfolder = './Img_Patches/{}/{}'.format(type,cls)\n",
        "        n = len(os.listdir(pfolder))\n",
        "        total = n\n",
        "        n_type += total\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "        class_weights[type]['{}'.format(cls)] = 1/n if n else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,n_type,0))\n",
        "    for loc in class_weights[type].keys():\n",
        "        class_weights[type][loc] *= type_pos\n",
        "    grand_total += n_type\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,grand_total,0))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 106 = 106 positive + 0 negative\n",
            "total_all_other: 533 = 533 positive + 0 negative\n",
            "Total Serial: 745 = 745 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_1: 13 = 13 positive + 0 negative\n",
            "total_2: 20 = 20 positive + 0 negative\n",
            "total_all_other: 255 = 255 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_1: 305 = 305 positive + 0 negative\n",
            "total_2: 431 = 431 positive + 0 negative\n",
            "total_all_other: 2491 = 2491 positive + 0 negative\n",
            "Total Streak: 3227 = 3227 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 4260 = 4260 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vb9BMr39R9LH"
      },
      "source": [
        "### **Since we want to focus on 'C1' and 'C2-3', let's downsample again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31ZsitULR-Ap",
        "outputId": "177e9d7e-43be-41a1-9c40-b4911af1ff74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pfolder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n = len(os.listdir(pfolder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority and key not in ['1','2']:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Img_Patches/{}/{}/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pfolder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n = len(os.listdir(pfolder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    class_weights[train_type]['{}'.format(cls)] = 1/n if n else 0\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))\n",
        "for loc in class_weights[train_type].keys():\n",
        "    class_weights[train_type][loc] *= type_pos"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 106 = 106 positive + 0 negative\n",
            "total_all_other: 533 = 533 positive + 0 negative\n",
            "Total Serial: 745 = 745 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 106 = 106 positive + 0 negative\n",
            "total_all_other: 106 = 106 positive + 0 negative\n",
            "Total Serial: 318 = 318 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pnI1G0ywLOp"
      },
      "source": [
        "#### **Since we still have slightly imbalanced training data, we have set different class weights to give more importance to the minority classes:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zCcbwQX7mKQq",
        "outputId": "b92c67c7-5b1f-484d-f558-6a29ceeb4409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print('Class Weights:', str(json.dumps(class_weights['Serial'], indent=2, default=str)))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Weights: {\n",
            "  \"1\": 0.0,\n",
            "  \"2\": 0.0,\n",
            "  \"all_other\": 0.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2gctgbsF-fF"
      },
      "source": [
        "### **Let's downsample positive majority classes in validation ('Control')patches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJETu-bSF_R8",
        "outputId": "e6054864-a42f-43a6-8624-59b33f6f5fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "val_type = 'Control'\n",
        "print('Before downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(val_type,cls)\n",
        "    n = len(os.listdir(pos_folder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,n_type,0))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Img_Patches/{}/{}/'.format(val_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(val_type,cls)\n",
        "    n = len(os.listdir(pos_folder))\n",
        "    total = n\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n if n else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,n_type,0))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling validation patches:\n",
            "total_1: 13 = 13 positive + 0 negative\n",
            "total_2: 20 = 20 positive + 0 negative\n",
            "total_all_other: 255 = 255 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_1: 13 = 13 positive + 0 negative\n",
            "total_2: 13 = 13 positive + 0 negative\n",
            "total_all_other: 13 = 13 positive + 0 negative\n",
            "Total Control: 39 = 39 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GNJ7lSKQpdOT"
      },
      "source": [
        "#### **Let's build image generators, using keras.preprocessing.image.ImageDataGenerator, rescaling image pixel values from [0,  255] to [0, 1]:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UANfdA6IUFKt",
        "outputId": "416d8033-cb5d-4efa-da98-245a30a5d7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "c1_pos_folder = './Img_Patches/Serial/1'\n",
        "img = plt.imread(c1_pos_folder + '/' + os.listdir(c1_pos_folder)[:5][0])\n",
        "img_size = img.shape\n",
        "train_batch_size = 32\n",
        "val_batch_size = 64\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"For training:\")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        './Img_Patches/Serial',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=train_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        "\n",
        "print(\"\\nFor validation:\")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        './Img_Patches/Control',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=val_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For training:\n",
            "Found 318 images belonging to 3 classes.\n",
            "\n",
            "For validation:\n",
            "Found 39 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44ZzUGXvwqUn"
      },
      "source": [
        "#### **Let's check what is the training generator's index for each class, so we can correclty set up the class weights:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSHQNC2awn84",
        "outputId": "3e5da8ca-7975-4a1f-8a9d-efbf2a0fdd74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print('train_generator.class_indices:', str(json.dumps(train_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_generator.class_indices: {\n",
            "  \"1\": 0,\n",
            "  \"2\": 1,\n",
            "  \"all_other\": 2\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o30MT4e2O7gk"
      },
      "source": [
        "#### **Let's set up the class weights in correct order:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2gsR9n6w2vm",
        "outputId": "1aa88e12-2766-4180-9676-f5d3c342cdba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "serial_pos_weights = []\n",
        "for cls in classes:\n",
        "    serial_pos_weights.append(class_weights['Serial']['{}'.format(cls)])\n",
        "print('original class weights dictionary:')\n",
        "print(str(json.dumps(class_weights['Serial'], indent=2, default=str)))\n",
        "print('class weights for generator, re-arranging indexes:')\n",
        "print(str(json.dumps(serial_pos_weights, indent=2, default=str)))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original class weights dictionary:\n",
            "{\n",
            "  \"1\": 0.0,\n",
            "  \"2\": 0.0,\n",
            "  \"all_other\": 0.0\n",
            "}\n",
            "class weights for generator, re-arranging indexes:\n",
            "[\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gdXBPBsVxA39"
      },
      "source": [
        "## **Let's build our Base Model. We will use Resnet:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AjNmNKdHw4tM"
      },
      "source": [
        "#### **First. let's create a function to build a residual block.**\n",
        "\n",
        "#### We will use the residual block proposed in  [ResNetV2](https://arxiv.org/pdf/1603.05027.pdf) and will implement it by ourselves:\n",
        "\n",
        "\n",
        ">![Google's logo](https://camo.githubusercontent.com/7ae470c333cd76078e1c669055ad98bcedaf523f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f61313536613563322d303236622d646535352d613666622d6534666131373732623432632e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-DqZH20w3ji",
        "colab": {}
      },
      "source": [
        "def res_block(X, filters, kernel_size=(3,3), l2_reg=1e-6, residual=True,\n",
        "              first=False, subsampling=False):\n",
        "    \"\"\"\n",
        "    Function to build a residual block as proposed in ResNetV2:\n",
        "             https://arxiv.org/pdf/1603.05027.pdf:\n",
        "    :param X: The input to the residual block\n",
        "    :param filters: Integer. Number of filters / channels in the output\n",
        "    :param kernel_size: Tuple (Int, Int). kernel size for convolution operations\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param residual: Boolean. True if residual block. Otherwise 'plain' block.\n",
        "    :param first: Boolean. True if first residual block -> ZeroPad and Maxpool.\n",
        "    :param subsampling: Boolean. True if subsampling within the residual block.\n",
        "    :return: the addition output from the residual block proposed in ResNetV2:\n",
        "              https://arxiv.org/pdf/1603.05027.pdf\n",
        "    \"\"\"\n",
        "    bn = BatchNormalization()(X)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "    \n",
        "    if first: #The first layer is subsampled with Maxpool\n",
        "      pad = ZeroPadding2D(padding=(1, 1))(relu)\n",
        "      relu = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(pad)    \n",
        "    \n",
        "    if subsampling: #Resnet reduces size just by using stride=2 instead of pool\n",
        "      #Here we will reduce the size (subsample) by using stride 2 \n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(2,2), padding='same',\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                         kernel_initializer = glorot_uniform(0),\n",
        "                         bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        #To be able to add, we also need to reduce size of input\n",
        "        #For this, we will just use a 1x1 Conv2D with stride 2  \n",
        "        res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "    else: #No subsampling, same size as input\n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(1,1), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        if first: #The first layer is subsampled with Maxpool so, resize X\n",
        "          #For this, we will just use a 1x1 Conv2D with stride 2\n",
        "          res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "        else:\n",
        "          res = X\n",
        "    bn = BatchNormalization()(conv_1)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    conv_2 = Conv2D(filters, kernel_size, padding='same',\n",
        "                       kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                       kernel_initializer = glorot_uniform(0),\n",
        "                       bias_initializer = glorot_uniform(0))(relu)\n",
        "    if residual:\n",
        "      add = keras.layers.add([res, conv_2])\n",
        "      return add\n",
        "    else:\n",
        "      return conv_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0K3-02NNUXOV"
      },
      "source": [
        "#### **Second, let's create a function to build a Resnet network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32s7rai-609p",
        "colab": {}
      },
      "source": [
        "#from keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
        "\n",
        "def make_resnet(img_size, n_classes, layers_per_group, n_filters, \n",
        "                    kernel_sizes, l2_reg=1e-6, optimizer=keras.optimizers.SGD,\n",
        "                    lr=1e-1, decay=1e-4, momentum=0.9, residual=True):\n",
        "  \n",
        "    \"\"\"\n",
        "    Function to build ResNet network, but with some user defined parameters.\n",
        "        \n",
        "        ResNet network input size is (224,224,3) then it starts with one \n",
        "        convolution layer, (7x7x64, stride 2) followed by maxpool (3x3, stride2) \n",
        "        then it will build 4 layer groups and the user will define the number of\n",
        "        residual layers per group.\n",
        "        \n",
        "        As example, ResNet34, after the first convolution layer, it has 4 groups\n",
        "        of layers with the following number of residual 'layers_per_group':\n",
        "        [6,8,12,6]\n",
        "        Because the residual connections are made between pair of layers, the\n",
        "        number of layers for each group must be a pair number.\n",
        "        \n",
        "        The user will also be able to define the number of filters and size of\n",
        "        each filter, independently for each group of layers. All layers in the\n",
        "        same group group will have the same number of filters and each filter\n",
        "        within the group will have the same size.\n",
        "        \n",
        "        Same as ResNet, an average pooling layer follows after the 4 groups\n",
        "        of layers.\n",
        "    \n",
        "    :param img_size: Size of input image, in the form: (size, size, #channels)\n",
        "    :param n_classes: Integer. Number of classes for classification.\n",
        "    :param layers_per_group: List with # of layers per group (e.g.[6,8,12,6])\n",
        "    :param n_filters: List of length 4, with number of filters for each group of\n",
        "                      layers.\n",
        "    :param kernel_sizes: List of length 4, with kernel sizes tuples for each \n",
        "                          group of layers\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param optimizer: A keras optimizer from keras.optimizers\n",
        "    :param lr: Float. learning rate for the optimizer.\n",
        "    :param decay: Float. learning rate decay for the optimizer.\n",
        "    :param momentum: Float. Momentum for SGD if optimizer=keras.optimizers.SGD\n",
        "    :param residual: Boolean. True if residual net. Otherwise 'plain' net.\n",
        "    :return: CNN with residual connections, similar to ResNet32\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(layers_per_block) != len(n_filters) or len(layers_per_block) !=\\\n",
        "        len(kernel_sizes) or len(n_filters) != len(n_filters):\n",
        "        e = \"Length of 'layers_per_block', 'n_filters' and 'kernel_sizes'\" +\\\n",
        "        \" must be the same\"\n",
        "        raise Exception(e)  \n",
        "\n",
        "    for layers in layers_per_block:\n",
        "      if layers % 2 == 1:\n",
        "        e = \"Number of 'layers_per_block' must be even/pair numbers\"\n",
        "        raise Exception(e)\n",
        "      \n",
        "    inputs = Input(shape=img_size)\n",
        "    \n",
        "    n_filters_conv1 = 64\n",
        "    kernel_sizes_conv1 = (5,5) # kernel size for very first conv layer\n",
        "    strides_conv1 = (2,2)\n",
        "    \n",
        "    conv1 = Conv2D(n_filters_conv1, kernel_sizes_conv1, strides=strides_conv1,\n",
        "                   padding='same', kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(inputs)\n",
        "    \n",
        "    layer_count = 1 # counter for the number of layers\n",
        "    for i in range(len(layers_per_block)):\n",
        "      for j in range(int(layers_per_block[i]/2)):\n",
        "        if j == 0:\n",
        "          if i == 0:\n",
        "            add = res_block(conv1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, first=True)\n",
        "            #add = res_block(conv0_1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "            #                residual, subsampling=True)\n",
        "          else:\n",
        "            add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, subsampling=True)\n",
        "        else:\n",
        "          add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                          residual)\n",
        "          \n",
        "    bn = BatchNormalization()(add)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    flat = GlobalAveragePooling2D()(relu)\n",
        "\n",
        "    out = Dense(n_classes, activation='softmax',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                      kernel_initializer=glorot_uniform(0),\n",
        "                      bias_initializer=glorot_uniform(0))(flat)\n",
        "\n",
        "    res_cnn = Model(inputs=inputs, outputs=out)\n",
        "\n",
        "    if optimizer == keras.optimizers.Nadam:\n",
        "        res_cnn.compile(optimizer(lr=lr, schedule_decay=decay),\n",
        "                    \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    elif optimizer == keras.optimizers.SGD:\n",
        "        res_cnn.compile(optimizer(lr=lr, momentum=momentum, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    else:\n",
        "        res_cnn.compile(optimizer(lr=lr, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    return res_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m21vIzndbOIZ"
      },
      "source": [
        "### **Let's build a ResNet18:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rLvSCOTsAuKF",
        "outputId": "afd1a18c-f27a-400a-ed44-945380c229c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classes = list(iter(train_generator.class_indices))\n",
        "n_classes = len(classes)\n",
        "layers_per_block = [4, 4, 4, 4] #18 layers total with first conv and last FC\n",
        "n_filters = [64, 128, 256, 512]\n",
        "kernel_sizes = [(3,3), (3,3), (3,3), (3,3)]\n",
        "l2_reg = 0.1\n",
        "optimizer = RMSprop # Adamax, RMSprop, Adam (No: Nadam, SGD)\n",
        "lr = 1e-3\n",
        "decay = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "res_cnn = make_resnet(img_size, n_classes, layers_per_block, n_filters,\n",
        "                       kernel_sizes, l2_reg, optimizer, lr, decay, momentum)\n",
        "res_cnn.summary()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 128, 128, 64) 4864        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 128, 128, 64) 256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 128, 128, 64) 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 130, 130, 64) 0           activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 64)   0           zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 64, 64, 64)   36928       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 64, 64, 64)   256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 64, 64, 64)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 64, 64, 64)   4160        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 64, 64, 64)   36928       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 64)   0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 64, 64, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 64, 64, 64)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 64, 64, 64)   36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 64, 64, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 64, 64, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 64, 64, 64)   36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 64, 64, 64)   0           add_17[0][0]                     \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 64, 64, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 64, 64, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 32, 32, 128)  73856       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 32, 32, 128)  512         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 32, 32, 128)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 32, 32, 128)  8320        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 32, 32, 128)  147584      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 128)  0           conv2d_50[0][0]                  \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 128)  512         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 32, 32, 128)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 32, 32, 128)  147584      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 32, 32, 128)  512         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 32, 32, 128)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 32, 32, 128)  147584      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 128)  0           add_19[0][0]                     \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 32, 32, 128)  512         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 32, 32, 128)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 256)  295168      activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 256)  1024        conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 256)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 256)  33024       add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 16, 16, 256)  590080      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 256)  0           conv2d_55[0][0]                  \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 256)  1024        add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 256)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 16, 16, 256)  590080      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 256)  1024        conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 256)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 16, 16, 256)  590080      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 256)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 256)  1024        add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 256)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 512)    1180160     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 512)    2048        conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 512)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 512)    131584      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 512)    2359808     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 8, 8, 512)    0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 512)    2048        add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 512)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 512)    2359808     activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 512)    2048        conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 512)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 512)    2359808     activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 512)    0           add_23[0][0]                     \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 512)    2048        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 512)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 512)          0           activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 3)            1539        global_average_pooling2d_3[0][0] \n",
            "==================================================================================================\n",
            "Total params: 11,188,419\n",
            "Trainable params: 11,180,611\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Snp4w6_wHP_"
      },
      "source": [
        "#### **Let's mount our GoogleDrive to download the best model (We have to authenticate again):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7NpUL6XwImK",
        "outputId": "3cc4abe4-24b0-4304-e7bc-9c6ef018865b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUL3HGj4dyft"
      },
      "source": [
        "### **Let's train and validate our Base Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDWJ135lVCbR",
        "outputId": "821e5b2e-84a7-4eaa-9d9a-b91e3ab70fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## fully balanced training\n",
        "## Rotate every 45\n",
        "## 'C1' vs 'C2-3' vs 'all_other' - Downsampling training\n",
        "\n",
        "## 128x128, stride_60,\n",
        "##min_pos_pix_1250, mivalpos_1024\n",
        "## ReduceLROnPlateau(monitor='val_loss'... )\n",
        "## train_batch_32, opt_RMSprop, Kernel_3x3:\n",
        "\n",
        "epochs = 80\n",
        "\n",
        "train_steps = train_generator.n//train_generator.batch_size\n",
        "val_steps = val_generator.n//val_generator.batch_size\n",
        "\n",
        "# Callbacks:\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, \n",
        "                                   verbose=1, mode='min', min_lr=1e-9)\n",
        "EarlyStop = EarlyStopping(monitor='val_acc', patience=70, verbose=1,\n",
        "                          min_delta=0, mode='max')\n",
        "checkpoint = ModelCheckpoint('base_model.h5', monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint, EarlyStop] #order matters!\n",
        "\n",
        "#res_cnn.load_weights('base_model.h5')\n",
        "\n",
        "history = res_cnn.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
        "                            validation_data=val_generator,\n",
        "                            validation_steps=val_steps, epochs=epochs,\n",
        "                            verbose=1, callbacks=callbacks_list, shuffle=False,\n",
        "                            class_weight=serial_pos_weights)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "9/9 [==============================] - 5s 539ms/step - loss: 185.3959 - acc: 0.5174 - val_loss: 89.2891 - val_acc: 0.5641\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.56410, saving model to base_model.h5\n",
            "Epoch 2/80\n",
            "9/9 [==============================] - 1s 101ms/step - loss: 62.6343 - acc: 0.5486 - val_loss: 40.0826 - val_acc: 0.5128\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.56410\n",
            "Epoch 3/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 29.7098 - acc: 0.5671 - val_loss: 20.7573 - val_acc: 0.3590\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.56410\n",
            "Epoch 4/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 16.3715 - acc: 0.6049 - val_loss: 15.3207 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.56410\n",
            "Epoch 5/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 10.6225 - acc: 0.5139 - val_loss: 9.3575 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.56410\n",
            "Epoch 6/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 7.2887 - acc: 0.5795 - val_loss: 6.9135 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.56410\n",
            "Epoch 7/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 5.3761 - acc: 0.6118 - val_loss: 5.0023 - val_acc: 0.4872\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.56410\n",
            "Epoch 8/80\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 4.1565 - acc: 0.6355 - val_loss: 4.9258 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.56410\n",
            "Epoch 9/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 3.4303 - acc: 0.6233 - val_loss: 4.1915 - val_acc: 0.4872\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.56410\n",
            "Epoch 10/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 2.9334 - acc: 0.6533 - val_loss: 3.6803 - val_acc: 0.5128\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.56410\n",
            "Epoch 11/80\n",
            "9/9 [==============================] - 1s 95ms/step - loss: 2.5438 - acc: 0.6493 - val_loss: 2.5045 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.56410 to 0.66667, saving model to base_model.h5\n",
            "Epoch 12/80\n",
            "9/9 [==============================] - 1s 101ms/step - loss: 2.4162 - acc: 0.6229 - val_loss: 2.7327 - val_acc: 0.5128\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.66667\n",
            "Epoch 13/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 2.2760 - acc: 0.5756 - val_loss: 5.0099 - val_acc: 0.4872\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.66667\n",
            "Epoch 14/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 2.4748 - acc: 0.4544 - val_loss: 2.3198 - val_acc: 0.4615\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.66667\n",
            "Epoch 15/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 2.1645 - acc: 0.5115 - val_loss: 2.5790 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.66667\n",
            "Epoch 16/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 1.9156 - acc: 0.5789 - val_loss: 2.2497 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.66667\n",
            "Epoch 17/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.8410 - acc: 0.5777 - val_loss: 2.1384 - val_acc: 0.4103\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.66667\n",
            "Epoch 18/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 1.6689 - acc: 0.6155 - val_loss: 2.6584 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.66667\n",
            "Epoch 19/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.6813 - acc: 0.5916 - val_loss: 2.0283 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.66667\n",
            "Epoch 20/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.5845 - acc: 0.6063 - val_loss: 6.1402 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.66667\n",
            "Epoch 21/80\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 1.5379 - acc: 0.6181 - val_loss: 2.5801 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.66667\n",
            "Epoch 22/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 1.3719 - acc: 0.6789 - val_loss: 2.0671 - val_acc: 0.4615\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0008500000403728336.\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.66667\n",
            "Epoch 23/80\n",
            "9/9 [==============================] - 1s 100ms/step - loss: 1.3891 - acc: 0.6088 - val_loss: 2.7650 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.66667\n",
            "Epoch 24/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.3829 - acc: 0.6394 - val_loss: 2.8116 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.66667\n",
            "Epoch 25/80\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 1.2274 - acc: 0.6470 - val_loss: 2.2882 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0007225000590551645.\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.66667\n",
            "Epoch 26/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.1805 - acc: 0.7054 - val_loss: 2.4565 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.66667\n",
            "Epoch 27/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 1.2338 - acc: 0.6693 - val_loss: 2.6354 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.66667\n",
            "Epoch 28/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.2919 - acc: 0.6194 - val_loss: 2.8959 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006141250254586339.\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.66667\n",
            "Epoch 29/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 1.1277 - acc: 0.6711 - val_loss: 2.4926 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.66667\n",
            "Epoch 30/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 1.0723 - acc: 0.7365 - val_loss: 2.5571 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.66667\n",
            "Epoch 31/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 1.0105 - acc: 0.7569 - val_loss: 2.5355 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005220062914304435.\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.66667\n",
            "Epoch 32/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.9898 - acc: 0.7551 - val_loss: 2.5483 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.66667\n",
            "Epoch 33/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.9345 - acc: 0.7896 - val_loss: 3.2920 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.66667\n",
            "Epoch 34/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.9825 - acc: 0.7412 - val_loss: 2.9796 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00044370535761117935.\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.66667\n",
            "Epoch 35/80\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 0.8885 - acc: 0.8111 - val_loss: 3.2110 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.66667\n",
            "Epoch 36/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.8908 - acc: 0.8148 - val_loss: 2.9774 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.66667\n",
            "Epoch 37/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 0.8905 - acc: 0.7757 - val_loss: 3.6814 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00037714955396950245.\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.66667\n",
            "Epoch 38/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.8934 - acc: 0.7662 - val_loss: 2.9094 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.66667\n",
            "Epoch 39/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.8277 - acc: 0.8044 - val_loss: 3.3270 - val_acc: 0.2821\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.66667\n",
            "Epoch 40/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.7992 - acc: 0.8252 - val_loss: 3.0165 - val_acc: 0.2564\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0003205771208740771.\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.66667\n",
            "Epoch 41/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.7646 - acc: 0.8438 - val_loss: 3.2103 - val_acc: 0.2564\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.66667\n",
            "Epoch 42/80\n",
            "9/9 [==============================] - 1s 95ms/step - loss: 0.8014 - acc: 0.8005 - val_loss: 2.8943 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.66667\n",
            "Epoch 43/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.7499 - acc: 0.8596 - val_loss: 3.4403 - val_acc: 0.2308\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002724905527429655.\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.66667\n",
            "Epoch 44/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.8374 - acc: 0.7901 - val_loss: 3.5586 - val_acc: 0.2308\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.66667\n",
            "Epoch 45/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.6696 - acc: 0.8880 - val_loss: 3.5251 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.66667\n",
            "Epoch 46/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.7315 - acc: 0.8602 - val_loss: 3.4060 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00023161696735769509.\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.66667\n",
            "Epoch 47/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.7389 - acc: 0.8381 - val_loss: 3.2591 - val_acc: 0.1282\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.66667\n",
            "Epoch 48/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.6516 - acc: 0.8986 - val_loss: 3.1853 - val_acc: 0.1282\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.66667\n",
            "Epoch 49/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.7029 - acc: 0.8708 - val_loss: 3.2609 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00019687442472786642.\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.66667\n",
            "Epoch 50/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.6977 - acc: 0.8773 - val_loss: 3.3967 - val_acc: 0.1282\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.66667\n",
            "Epoch 51/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.6345 - acc: 0.9097 - val_loss: 2.9498 - val_acc: 0.1795\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.66667\n",
            "Epoch 52/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.6323 - acc: 0.8923 - val_loss: 3.6221 - val_acc: 0.1282\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00016734325545257888.\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.66667\n",
            "Epoch 53/80\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 0.6147 - acc: 0.9088 - val_loss: 2.8563 - val_acc: 0.1795\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.66667\n",
            "Epoch 54/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.6772 - acc: 0.8880 - val_loss: 3.0280 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.66667\n",
            "Epoch 55/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.6108 - acc: 0.8921 - val_loss: 2.6355 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00014224176775314845.\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.66667\n",
            "Epoch 56/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.5883 - acc: 0.9047 - val_loss: 2.7933 - val_acc: 0.2051\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.66667\n",
            "Epoch 57/80\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 0.6291 - acc: 0.8814 - val_loss: 2.6999 - val_acc: 0.1795\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.66667\n",
            "Epoch 58/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.6583 - acc: 0.8847 - val_loss: 2.3770 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00012090550444554538.\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.66667\n",
            "Epoch 59/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 0.5523 - acc: 0.9366 - val_loss: 2.3710 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.66667\n",
            "Epoch 60/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.5638 - acc: 0.9299 - val_loss: 2.4287 - val_acc: 0.2308\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.66667\n",
            "Epoch 61/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.5888 - acc: 0.9132 - val_loss: 2.1597 - val_acc: 0.2821\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010276967877871357.\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.66667\n",
            "Epoch 62/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.5355 - acc: 0.9442 - val_loss: 2.0354 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.66667\n",
            "Epoch 63/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.5359 - acc: 0.9477 - val_loss: 1.9578 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.66667\n",
            "Epoch 64/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.5208 - acc: 0.9477 - val_loss: 1.9851 - val_acc: 0.2564\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.66667\n",
            "Epoch 65/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.5226 - acc: 0.9514 - val_loss: 1.7785 - val_acc: 0.3590\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.66667\n",
            "Epoch 66/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.5320 - acc: 0.9370 - val_loss: 1.7533 - val_acc: 0.3590\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.66667\n",
            "Epoch 67/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.4856 - acc: 0.9577 - val_loss: 1.9908 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.66667\n",
            "Epoch 68/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 0.4908 - acc: 0.9442 - val_loss: 1.6957 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.66667\n",
            "Epoch 69/80\n",
            "9/9 [==============================] - 1s 95ms/step - loss: 0.5009 - acc: 0.9514 - val_loss: 1.6351 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.66667\n",
            "Epoch 70/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.4503 - acc: 0.9789 - val_loss: 1.8669 - val_acc: 0.2564\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.66667\n",
            "Epoch 71/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.5286 - acc: 0.9201 - val_loss: 3.2495 - val_acc: 0.3590\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.66667\n",
            "Epoch 72/80\n",
            "9/9 [==============================] - 1s 100ms/step - loss: 0.4448 - acc: 0.9757 - val_loss: 3.0401 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 8.735422634345013e-05.\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.66667\n",
            "Epoch 73/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.4517 - acc: 0.9548 - val_loss: 3.2669 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.66667\n",
            "Epoch 74/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.4426 - acc: 0.9757 - val_loss: 3.8032 - val_acc: 0.3590\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.66667\n",
            "Epoch 75/80\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.4214 - acc: 0.9792 - val_loss: 1.9312 - val_acc: 0.2564\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 7.425108960887882e-05.\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.66667\n",
            "Epoch 76/80\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 0.4292 - acc: 0.9722 - val_loss: 3.7057 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.66667\n",
            "Epoch 77/80\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 0.4256 - acc: 0.9748 - val_loss: 3.6517 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.66667\n",
            "Epoch 78/80\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.4089 - acc: 0.9826 - val_loss: 3.6510 - val_acc: 0.4103\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 6.31134258583188e-05.\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.66667\n",
            "Epoch 79/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.4250 - acc: 0.9826 - val_loss: 4.0968 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.66667\n",
            "Epoch 80/80\n",
            "9/9 [==============================] - 1s 93ms/step - loss: 0.3857 - acc: 0.9896 - val_loss: 4.0836 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.66667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BeQ6tt-mwUW3"
      },
      "source": [
        "#### **Let's download the best model to our 'Capstone' folder in GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SA1xKG_gwVP4",
        "outputId": "18b3daf0-4f09-44f0-cab8-641045172036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "try:\n",
        "  files.download('base_model.h5')\n",
        "except:\n",
        "  print(\"Not done\")\n",
        "  pass\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 38780, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 36348, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MRz_1oJzJRFH"
      },
      "source": [
        "#### **Let's download the training history to a local file:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5Y5GUUhcyAt",
        "colab": {}
      },
      "source": [
        "for k,v in history.history.items():\n",
        "  history.history[k] = str(v)\n",
        "\n",
        "with open('history_1-2.json', 'w') as f:\n",
        "    json.dump(history.history, f)\n",
        "\n",
        "try:\n",
        "    time.sleep(3) # To avoid warning when downloading various files at once\n",
        "    files.download('history_1-2.json')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxEpvYPVcyUb",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "with open('history_1-2.json') as f:\n",
        "    history_dict = json.load(f)\n",
        "    \n",
        "for k,v in history_dict.items():\n",
        "  history_dict[k] = json.loads(v)\n",
        "\n",
        "history_df = pd.DataFrame(history_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_eAC27OxKOLY"
      },
      "source": [
        "#### **Let's plot training and validation loss vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFs4JAkn-vtq",
        "outputId": "15e7faef-a30d-4f31-8538-cb4a7df8f73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "loss = history_df[['loss','val_loss']]\n",
        "loss.columns = ['train_loss', 'val_loss']\n",
        "loss.plot(figsize=(10, 6), title='Loss vs epochs')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff88ad65898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAF1CAYAAAAqdaQaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VSFAqg-gKlR0"
      },
      "source": [
        "#### **Let's plot training and validation accuracy vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zux8aIdQN7zN",
        "outputId": "690a41cc-ac7d-4d56-b134-3c1dcac104b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "acc = history_df[['val_acc','acc']]\n",
        "acc.columns = ['val_acc', 'train_acc']\n",
        "acc.plot(figsize=(10, 6), title='Val acc & Train acc vs epochs')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff889d9b748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8RiTO3yLDXy"
      },
      "source": [
        "#### **Let's plot learning rate vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o2ggKlccOAHA",
        "outputId": "9a771542-8259-4142-b9f1-d73ba035814a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "lr = history_df[['lr']]\n",
        "lr.plot(figsize=(10, 6), title='Learning rate vs epochs')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff888586710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAF1CAYAAABChiYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZxddX3/8ddn9mQyk3WyTkImQBIC\nJCwhgIoguIAouKAGrUWKRa20/bV2gdZftVb7K7V1Ry0VkFplcSWCYlFE3ICEHQKBkAQSyEb2hOzz\n/f1xT2Q6zprM3HPvzOv5eMwj955zvud+zklu8s75fs/3REoJSZIk5a8i7wIkSZJUYDCTJEkqEQYz\nSZKkEmEwkyRJKhEGM0mSpBJhMJMkSSoRBjNJuYuIH0fERXnXMZhFxNcj4pN51yENdgYzaRCLiBUR\n8dq860gpnZNSuj7vOgAi4q6IeH/edUganAxmkvpVRFTlXcMBpVSLJHXEYCapQxHxpoh4KCI2R8Rv\nImJ2m3WXR8QzEbEtIhZHxFvbrHtfRPw6Ij4bERuAj2fLfhUR/xYRmyJieUSc06bN765S9WDbloi4\nO/vsn0bEVRHx350cwxkRsSoi/jYi1gDXRcTIiLg1ItZn+781Ipqz7T8FnAZ8KSK2R8SXsuUzI+KO\niNgYEUsi4p2dfN67ImJRu2V/ERELstdvzM7Xtoh4PiL+qovz/0cR8URW408i4rA261JE/FlELIuI\nFyPi0xFRka2riIiPRsSzEbEuIv4rIoa3afuq7Pdzc0SsjIj3tfnYkRFxW1bfvRFxeNYmst/PdRGx\nNSIejYhjOqtd0sEzmEn6PRFxPHAt8AFgNPAfwIKIqM02eYZCgBkO/CPw3xExoc0uTgaWAeOAT7VZ\ntgQYA/wrcE1ERCcldLXtt4D7sro+Dry3m8MZD4wCDgMupfD33nXZ+ynATuBLACmlvwd+CVyWUhqW\nUrosIuqBO7LPHQvMB74cEbM6+KwfAjMi4sg2y96dtQW4BvhASqkBOAa4s6OCI+J84O+AtwFNWU03\ntNvsrcBc4ATgfOCPsuXvy35eA0wDhh04vizc/Rj4Yrbf44CH2uxzPoXfz5HAUl7+vXs98GpgOoXf\n83cCGzqqXdKhMZhJ6silwH+klO5NKe3Pxn/tBk4BSCl9O6X0QkqpNaV0E/A0MK9N+xdSSl9MKe1L\nKe3Mlj2bUvrPlNJ+4HpgAoXg1pEOt42IKcBJwD+klPaklH4FLOjmWFqBj6WUdqeUdqaUNqSUvptS\neimltI1C+Di9i/ZvAlaklK7LjudB4LvAO9pvmFJ6CbgFuBAgC2gz29S4F5gVEY0ppU0ppQc6+cwP\nAv8vpfRESmkf8M/AcW2vmgFXppQ2ppSeAz534DOB9wCfSSktSyltB64A5mfduO8GfppSuiGltDc7\nF22D2fdTSvdln/lNCsHtQN0N2bFEVtfqLs6ZpINkMJPUkcOAj2TdXZsjYjMwGZgIEBF/2KabczOF\nqz9j2rRf2cE+1xx4kQUYKFzN6Uhn204ENrZZ1tlntbU+pbTrwJuIGBoR/5F19W0F7gZGRERlJ+0P\nA05udy7eQ+FKXEe+xcsh6d3AD9rU+3bgjcCzEfGLiDi1i8/8fJvP2wgEMKnNNm2P+1my35vs12fb\nrauiEIInU7ja2Zk1bV6/RPb7k1K6k8JVt6uAdRFxdUQ0drEfSQfJYCapIyuBT6WURrT5GZpSuiG7\navOfwGXA6JTSCOAxCsHhgNRPda0GRkXE0DbLJnfTpn0tHwFmACenlBopdNHBy/W3334l8It252JY\nSulDnXzeHUBTRBxHIaAd6MYkpbQwpXQ+hS7RHwA3d7KPlRS6PNt+5pCU0m/abNP2uKcAL2SvX6AQ\n7Nqu2weszfZ7eCef2aWU0hdSSicCsyh0af71wexHUtcMZpKqI6KuzU8VheD1wYg4ORv4XR8R50ZE\nA1BPIbysB4iIiylcMet3KaVngUUUbiioya44vbmXu2mgMK5sc0SMAj7Wbv1aCmOzDrgVmB4R742I\n6uznpIg4qpMa9wLfBj5NYWzbHQBZve+JiOHZNlspdLN25KvAFRFxdNZ2eES07zr96yjcyDAZ+HPg\npmz5DcBfROEmiWEUukFvatM9+dqIeGdEVEXE6CxAdik73pMjohrYAezqonZJh8BgJulHFILKgZ+P\np5QWAX9MoftqE4WB4O8DSCktBv4d+C2FEHMs8Osi1vse4FQKg88/SSGQ7O5F+88BQ4AXgXuA29ut\n/zxwQXY35BeycWivpzAw/gUK3X1XArV07lvAa4FvZ4HogPcCK7Iu1A9mx/J7Ukrfzz7jxmzbx4Bz\n2m12C3A/hcH7t1G4sQAKN218g0IX7XIKIepPs/0+R6Er9SMUukcfAuZ0cRwHNFII65sodI1uoBA8\nJfWxSKm/ehwkqf9FxE3Akyml9le+BqyISMCRKaWledciqW95xUxSWcm61Q7P5us6m8JUET/Iuy5J\n6gvOgi2p3IwHvkdhHrNVwIeyKSwkqezZlSlJklQi7MqUJEkqEQYzSZKkEjEgxpiNGTMmTZ06Ne8y\nJEmSunX//fe/mFJq6mjdgAhmU6dOZdGiRXmXIUmS1K2IeLazdXZlSpIklQiDmSRJUokwmEmSJJWI\nATHGTJIkDR579+5l1apV7Nq1K+9SulRXV0dzczPV1dU9bmMwkyRJZWXVqlU0NDQwdepUIiLvcjqU\nUmLDhg2sWrWKlpaWHrezK1OSJJWVXbt2MXr06JINZQARwejRo3t9Vc9gJkmSyk4ph7IDDqZGg5kk\nSVIvDRs2rF/2azCTJEnqA/v27TvkffQomEXE2RGxJCKWRsTlHayvjYibsvX3RsTUNuuuyJYviYg3\ntFl+bUSsi4jH2u1rVETcERFPZ7+OPPjDkyRJ6j933XUXp512Gueddx6zZs065P11e1dmRFQCVwGv\nA1YBCyNiQUppcZvNLgE2pZSOiIj5wJXAuyJiFjAfOBqYCPw0IqanlPYDXwe+BPxXu4+8HPhZSulf\nshB4OfC3h3KQkiRpYPrHHz7O4he29uk+Z01s5GNvPrrH2z/wwAM89thjvbr7sjM9mS5jHrA0pbQM\nICJuBM4H2gaz84GPZ6+/A3wpCiPezgduTCntBpZHxNJsf79NKd3d9spau32dkb2+HriLboLZtl37\n+PmSdT04FKl7U0fX0zKmPu8yJEllYt68eX0SyqBnwWwSsLLN+1XAyZ1tk1LaFxFbgNHZ8nvatZ3U\nzeeNSymtzl6vAcZ1tFFEXApcClAz/gguvm5h90ci9cD4xjru+buz8i5DktQDvbmy1V/q6/vuP/Ml\nPcFsSilFROpk3dXA1QCzZh+fvvknryhqbRqYfvjwaq799XK2vLSX4UN7PlOzJEl9oSfB7Hlgcpv3\nzdmyjrZZFRFVwHBgQw/btrc2IiaklFZHxASg2z7KoTWVHD/FewR06F7cvodrf72c5Rt2cNzQEXmX\nI0kaZHpyV+ZC4MiIaImIGgqD+Re022YBcFH2+gLgzpRSypbPz+7abAGOBO7r5vPa7usi4JYe1Cj1\niWlNhcvRy1/cnnMlkqRStn174d+JM844g1tvvbXP9tttMEsp7QMuA34CPAHcnFJ6PCI+ERHnZZtd\nA4zOBvf/JYU7KUkpPQ7cTOFGgduBD2d3ZBIRNwC/BWZExKqIuCTb178Ar4uIp4HXZu+lopg8ciiV\nFcHy9TvyLkWSNAj1aIxZSulHwI/aLfuHNq93Ae/opO2ngE91sPzCTrbfADjyWrmoqapg8sghPPOi\nwUySVHzO/C+10zKm3itmkqRcGMykdlrGDGP5izsoDJOUJJWicvg7+mBqNJhJ7bQ01bNz737Wbt2d\ndymSpA7U1dWxYcOGkg5nKSU2bNhAXV1dr9qV9DxmUh6mZbP+L3txO+OH9+4LJUnqf83NzaxatYr1\n69fnXUqX6urqaG5u7lUbg5nUzoHHMS1/cQevOHxMztVIktqrrq7us0cglRq7MqV2xjfWUVdd4Q0A\nkqSiM5hJ7VRUBFNH17PcKTMkSUVmMJM6MK3JYCZJKj6DmdSBaWOG8dzGl9i7vzXvUiRJg4jBTOpA\ny5h69rUmVm3amXcpkqRBxGAmdaAle5j5svU+zFySVDwGM6kD09pMmSFJUrEYzKQOjBhaw8ih1Swz\nmEmSishgJnXCh5lLkorNYCZ14sDDzCVJKhaDmdSJaU31rNm6ix279+VdiiRpkDCYSZ048MzMFRu8\naiZJKg6DmdSJFu/MlCQVmcFM6sTU0Vkw8wYASVKRGMykTgypqWTSiCFeMZMkFY3BTOpCy5h65zKT\nJBWNwUzqQsuYepat305KKe9SJEmDgMFM6kLLmHq27trHxh178i5FkjQIGMykLhx4mLnjzCRJxWAw\nk7pw4GHmjjOTJBWDwUzqwqQRQ6iuDK+YSZKKwmAmdaGqsoIpo4Y6l5kkqSgMZlI3fJi5JKlYDGZS\nN6Y11bN8ww5aW50yQ5LUvwxmUjdaxtSzZ18rL2zZmXcpkqQBzmAmdWOaDzOXJBWJwUzqxoG5zJZ5\nA4AkqZ8ZzKRuNA2rZVhtlVfMJEn9zmAmdSMifJi5JKkoDGZSD7SMqWf5i9vzLkOSNMAZzKQeaBlT\nz6pNO9m9b3/epUiSBjCDmdQD05rqSQme2/BS3qVIkgYwg5nUAy0+zFySVAQGM6kHpjqXmSSpCAxm\nUg801lUzZlitDzOXJPUrg5nUQ9Oa6r1iJknqV1V5FyCVi2lj6vneA89zzud/2eu2YxtqufoPT6S2\nqrIfKpMkDRQGM6mH3jF3Mpte2kNr6l27jTv28Iun1rNkzTZmN4/on+IkSQOCwUzqoRMPG8l/vHdu\nr9stf3EHr/m3u3jSYCZJ6oZjzKR+NmXUUOqqK1iyZlvepUiSSpzBTOpnlRXBkWMbeGqtwUyS1DWD\nmVQEM8Y38KRXzCRJ3TCYSUUwc3wD67ftZuOOPXmXIkkqYQYzqQhmjG8A4Mk1W3OuRJJUygxmUhHM\nGFcIZt4AIEnqisFMKoKmhlpGDq02mEmSumQwk4ogIpgxvoEl3pkpSeqCwUwqkpnjG3lqzTZae/vo\nAEnSoGEwk4pkxvgGduzZz/Obd+ZdiiSpRPUomEXE2RGxJCKWRsTlHayvjYibsvX3RsTUNuuuyJYv\niYg3dLfPiDgrIh6IiIci4lcRccShHaJUGqaPO3Bnpt2ZkqSOdRvMIqISuAo4B5gFXBgRs9ptdgmw\nKaV0BPBZ4Mqs7SxgPnA0cDbw5Yio7GafXwHek1I6DvgW8NFDO0SpNByYMmOJU2ZIkjrRkytm84Cl\nKaVlKaU9wI3A+e22OR+4Pnv9HeCsiIhs+Y0ppd0ppeXA0mx/Xe0zAY3Z6+HACwd3aFJpGVZbRfPI\nISxZuz3vUiRJJaqqB9tMAla2eb8KOLmzbVJK+yJiCzA6W35Pu7aTsted7fP9wI8iYiewFTilBzVK\nZWHm+AavmEmSOlWKg///AnhjSqkZuA74TEcbRcSlEbEoIhatX7++qAVKB2vG+AaWrd/Bnn2teZci\nSSpBPQlmzwOT27xvzpZ1uE1EVFHogtzQRdsOl0dEEzAnpXRvtvwm4BUdFZVSujqlNDelNLepqakH\nhyHlb/q4Bva1Jp5Zb3emJOn39SSYLQSOjIiWiKihMJh/QbttFgAXZa8vAO5MKaVs+fzsrs0W4Ejg\nvi72uQkYHhHTs329Dnji4A9PKi0zxxeGT/oEAElSR7odY5aNGbsM+AlQCVybUno8Ij4BLEopLQCu\nAb4REUuBjRSCFtl2NwOLgX3Ah1NK+wE62me2/I+B70ZEK4Wg9kd9esRSjqY11VNdGU6ZIUnqUBQu\nbJW3uXPnpkWLFuVdhtQjZ3/ubiaOGMK17zsp71IkSTmIiPtTSnM7WleKg/+lAW3G+Aa7MiVJHTKY\nSUU2fVwDz2/eydZde/MuRZJUYgxmUpHNzJ4A8JRXzSRJ7RjMpCI78GgmbwCQJLVnMJOKbNKIITTU\nVjnOTJL0ewxmUpFFBNPHN7BkrcFMkvS/GcykHEwfV7gzcyBMVyNJ6jsGMykHM8c3sGXnXtZu3Z13\nKZKkEmIwk3Lw8g0AW3OuRJJUSgxmUg4OTJnhDQCSpLYMZlIORgytYVxjrcFMkvS/GMyknEwf552Z\nkqT/zWAm5WTm+AaeXredfftb8y5FklQiDGZSTmaMb2TPvlZWbHgp71IkSSXCYCblxBsAJEntGcyk\nnBwxdhgVAUucMkOSlDGYSTmpq65k6uh6H2YuSfodg5mUoxnjG3jKOzMlSZmqvAuQBrMZ4xu4/fE1\nfPmupVRE9KptXVUF8+dNoa66sp+qkyQVm8FMytEp00bzxTuX8q+3Lzmo9qOH1fLmORP7uCpJUl4M\nZlKOTpk2msWfeAOtvZzKbF9rKyd+8qc8vHKzwUySBhCDmZSz2qqD6YqsZNaERh55fkuf1yNJyo+D\n/6UyNad5OI89v4X9rSnvUiRJfcRgJpWp2c0jeGnPfp5Zvz3vUiRJfcRgJpWp2c3DAXhkld2ZkjRQ\nGMykMjWtaRj1NZU8smpz3qVIkvqIwUwqU5UVwTGThvOwV8wkacAwmEllbM7kETzxwlb27OvlfBuS\npJJkMJPK2LGThrNnf6uPdZKkAcJgJpWxOc0jAHjYcWaSNCAYzKQyNnnUEEYMreaRlY4zk6SBwGAm\nlbGI4NhJw30CgCQNEAYzqczNaR7BU2u3sXPP/rxLkSQdIoOZVOZmNw9nf2ti8WqvmklSuTOYSWVu\nzuTCDQA+AUCSyp/BTCpz4xrrGNtQazCTpAHAYCYNALObRzhlhiQNAAYzaQCY0zycZet3sHXX3rxL\nkSQdAoOZNADMzsaZPea0GZJU1gxm0gBw7KThgDcASFK5M5hJA8Co+homjxrCI44zk6SyZjCTBojZ\nzSO8YiZJZc5gJg0Qc5qHs2rTTjZs3513KZKkg2QwkwaIYydlE816A4AklS2DmTRAHNs8nAh41O5M\nSSpbBjNpgBhWW8XhTcO8AUCSypjBTBpAZk8azsOrtpBSyrsUSdJBMJhJA8js5uGs37abtVu9AUCS\nypHBTBpADjwBwOdmSlJ5MphJA8isCY1UVYTjzCSpTBnMpAGkrrqS6eManGhWksqUwUwaYOZMHs4j\n3gAgSWWpKu8CJPWt2c0juOG+lfzmmQ2Ma6ztVduKCKaOrqeiIvqpOklSVwxm0gBz/JTCDQDv+dq9\nB9X+o+cexftPm9aXJUmSeshgJg0wM8c38t+XnMyml/b0uu2nf7KEXz79osFMknLSo2AWEWcDnwcq\nga+llP6l3fpa4L+AE4ENwLtSSiuydVcAlwD7gT9LKf2kq31GRACfBN6RtflKSukLh3aY0uDyqiPH\nHFS7e5Zt4JaHXmDf/laqKh2CKknF1u3fvBFRCVwFnAPMAi6MiFntNrsE2JRSOgL4LHBl1nYWMB84\nGjgb+HJEVHazz/cBk4GZKaWjgBsP6Qgl9di8llFs372PJ1Zvy7sUSRqUevJf4nnA0pTSspTSHgpB\n6fx225wPXJ+9/g5wVnbl63zgxpTS7pTScmBptr+u9vkh4BMppVaAlNK6gz88Sb1xcstoAO5dviHn\nSiRpcOpJMJsErGzzflW2rMNtUkr7gC3A6C7adrXPw4F3RcSiiPhxRBzZUVERcWm2zaL169f34DAk\ndWf88DoOGz2U+5ZvzLsUSRqUSnEQSS2wK6U0F/hP4NqONkopXZ1SmptSmtvU1FTUAqWBbN7UUSxc\nsZHWVudBk6Ri60kwe57CmK8DmrNlHW4TEVXAcAo3AXTWtqt9rgK+l73+PjC7BzVK6iPzWkax6aW9\nLF2/Pe9SJGnQ6UkwWwgcGREtEVFDYTD/gnbbLAAuyl5fANyZCtOOLwDmR0RtRLQARwL3dbPPHwCv\nyV6fDjx1cIcm6WC8PM7M7kxJKrZug1k2Zuwy4CfAE8DNKaXHI+ITEXFettk1wOiIWAr8JXB51vZx\n4GZgMXA78OGU0v7O9pnt61+At0fEo8D/A97fN4cqqScmjxrC+MY6x5lJUg5iIDxPb+7cuWnRokV5\nlyENGH92w4Pcu3wD91xxFoUbrCVJfSUi7s/G0v+eUhz8Lyln81pGsXbrbp7b+FLepUjSoGIwk/R7\nTm4ZBTjOTJKKzWAm6fccMXYYo+pruHeZwUySislgJun3RAQnTR3JfSt8AoAkFZPBTFKH5rWMZuXG\nnbyweWfepUjSoGEwk9ShA+PMFq6wO1OSisVgJqlDR01oZFhtlTcASFIRGcwkdaiyIpg7daQTzUpS\nERnMJHVqXssolq7bzovbd+ddiiQNCgYzSZ06MM5skePMJKkoDGaSOnXspBHUVlU4zkySisRgJqlT\nNVUVnDDFcWaSVCwGM0ldmtcyisWrt7J11968S5GkAc9gJqlLJ7eMIiW4f8WmvEuRpAHPYCapS8dP\nGUlVRTjOTJKKwGAmqUtDaiqZ3Tyc+5b73ExJ6m8GM0ndmtcymkdWbWHnnv15lyJJA5rBTFK3Tp42\nin2tifuf3URra+r1jySpZ6ryLkBS6TvxsJFUBPzBNfceVPs/Pq2Fvz93Vh9XJUkDj8FMUrca66r5\n4oUnsHTd9l63vfPJtXz/wRe44pyjqKiIfqhOkgYOg5mkHjl39oSDatc8cggf+fbDLF69lWMmDe/j\nqiRpYHGMmaR+9erpTQDctWRdzpVIUukzmEnqV00NtRwzqZG7lqzPuxRJKnkGM0n97ozpY3nguU1s\necnHOklSVwxmkvrdGTOaaE3wy6VeNZOkrhjMJPW74yaPoLGuyu5MSeqGwUxSv6uqrOC06U384qn1\nTjgrSV0wmEkqijOmN7F+224Wr96adymSVLIMZpKK4vQZhWkzfvGU3ZmS1BmDmaSiGNtQx9ETG53P\nTJK6YDCTVDRnzGjigec2s2Wn02ZIUkcMZpKK5owZY9nfmvjV0y/mXYoklSSDmaSiOf5302bYnSlJ\nHTGYSSqaqsoKTjuyMG1GSk6bIUntGcwkFdXpM5pYt203T6zelncpklRyDGaSiuqM6YVpM+56yu5M\nSWrPYCapqMY21jFrQqOPZ5KkDhjMJBXdGTOauP/ZTWzd5bQZktSWwUxS0R2YNuPXTpshSf+LwUxS\n0R0/ZQQNtVV2Z0pSOwYzSUVXXVnBq44c47QZktSOwUxSLs6Y0cSarbt4co3TZkjSAVV5FyBpcDp9\n+lgAbl60kjfNntDr9pNHDmVsY11flyVJuTKYScrF+OF1HDOpket+vYLrfr2i1+0njxrCL/7qNVRU\nRN8XJ0k5MZhJys01F53EkoPoylz07Ca+8LOneXDlZk48bGQ/VCZJ+TCYScrNuMY6xh1Ed+RxU0bw\n1bue4bZHVhvMJA0oDv6XVHYa66p59fQmfvToalpbvatT0sBhMJNUlt40ewJrtu7iwZWb8i5FkvqM\nwUxSWTrrqLHUVFVw6yOr8y5FkvqMwUxSWWqoq+Z0uzMlDTAGM0ll602zJ7B2627uf87uTEkDg8FM\nUtk666hx1FRVcJvdmZIGCIOZpLI1rLaK18ywO1PSwGEwk1TWzp09kXXbdrPoWbszJZW/HgWziDg7\nIpZExNKIuLyD9bURcVO2/t6ImNpm3RXZ8iUR8YZe7PMLEbH94A5L0mBx1syx1FZVcNsjL+RdiiQd\nsm6DWURUAlcB5wCzgAsjYla7zS4BNqWUjgA+C1yZtZ0FzAeOBs4GvhwRld3tMyLmAk7nLalb9bVV\nnDlzLD96bA377c6UVOZ6csVsHrA0pbQspbQHuBE4v9025wPXZ6+/A5wVEZEtvzGltDultBxYmu2v\n031moe3TwN8c2qFJGizOnT2B9dt2s3DFxrxLkaRD0pNgNglY2eb9qmxZh9uklPYBW4DRXbTtap+X\nAQtSSl3eZhURl0bEoohYtH79+h4chqSB6syZY6mr9u5MSeWvpAb/R8RE4B3AF7vbNqV0dUppbkpp\nblNTU/8XJ6lkDa0pdGf+2O5MSWWuJ8HseWBym/fN2bIOt4mIKmA4sKGLtp0tPx44AlgaESuAoRGx\ntIfHImkQO/fYiby4fTf3Lbc7U1L56kkwWwgcGREtEVFDYTD/gnbbLAAuyl5fANyZUkrZ8vnZXZst\nwJHAfZ3tM6V0W0ppfEppakppKvBSdkOBJHXpNTObGFJdyW2PenempPLVbTDLxoxdBvwEeAK4OaX0\neER8IiLOyza7BhidXd36S+DyrO3jwM3AYuB24MMppf2d7bNvD03SYDK0poozjxrL7Y+tYd/+1rzL\nkaSDEoULW+Vt7ty5adGiRXmXISlnP350NR/65gN86/0n84ojxuRdjiR1KCLuTynN7WhdVbGLkaT+\ncsaMsQytqeTmRStpaqjtdfsRQ2sOqp0k9RWDmaQBY0hNJa89ahw/eOgFfvBQ78ea1VVX8Mu/OdNw\nJik3BjNJA8rH3jyLNxw9nkTvhmls3bmPv/v+o3z3gVV88PTD+6k6SeqawUzSgDJ6WC3nzp5wUG2/\n/+Aqblq4kg+8ehqFh5dIUnGV1ASzkpSn+SdNYfmLO7hnmXOhScqHwUySMm88dgINdVXcuPC5vEuR\nNEgZzCQpM6SmkrceP4kfP7aGzS/tybscSYOQwUyS2ph/0hT27Gvlew+0f/KcJPU/g5kktTFrYiNz\nmodz48LnGAgTcEsqLwYzSWpn/rwpPLV2Ow88tznvUiQNMgYzSWrnzXMmMrSmkhvv8yYAScVlMJOk\ndobVVnHenInc+shqtu3am3c5kgYRg5kkdWD+vCns3LufWw7i0U6SdLAMZpLUgTnNw5k5vsE5zSQV\nlcFMkjoQEVw4bwqPPb+Vx57fknc5kgYJg5kkdeItx02itqqCG7wJQFKRGMwkqRPDh1Zz7rETuOWh\nF3hpz768y5E0CBjMJKkL8+dNYfvufdz6yOq8S5E0CFTlXYAklbKTpo7k8KZ6/vueZ2keMaTX7YfU\nVHLc5BFERD9UJ2mgMZhJUhcigneffBj/dOti3v21ew9qH1e/90Ref/T4Pq5M0kBkMJOkblx06mEc\nN3k4+/b3/tmZf/Wdh/nKL57hdbPGedVMUrcMZpLUjarKCk48bNRBtb30tGn831se577lGzl52ug+\nrkzSQOPgf0nqRxecOJlR9Y/DrOMAABCySURBVDV89RfP5F2KpDJgMJOkfjSkppL3vWIqP1+ynifX\nbM27HEklzmAmSf3sD089jKE1lVz9i2V5lyKpxBnMJKmfjRhaw/yTprDg4Rd4fvPOvMuRVMIMZpJU\nBO8/rQWAr/3Sq2aSOmcwk6QimDhiCOcdN5Eb71vJph178i5HUokymElSkXzg1Yezc+9+vnHPs3mX\nIqlEGcwkqUhmjG/gzJlj+fpvVrBzz/68y5FUggxmklREHzz9cDbu2MO371+ZdymSSpDBTJKK6KSp\nIzlhygiuvnsZ+/a35l2OpBJjMJOkIooIPnj64azatJMfPbYm73IklRiflSlJRfbao8ZxeFM9X73r\nGV571FiC3j3cPALqqiv7qTpJeTKYSVKRVVQEH3j14fzNdx9h1j/8pNftqyqCf71gNm87obkfqpOU\nJ4OZJOXgbSdMYvf+Vnbs3tfrtrc+8gL//KMneP3R4xlW61/j0kDiN1qSclBVWcF7TznsoNqeMm00\nb7nq11x99zL+8nXT+7gySXly8L8klZnjJo/g3NkT+M+7l7Fu6668y5HUhwxmklSG/vr1M9i7v5XP\n/ezpvEuR1IcMZpJUhqaOqecPTjmMmxauZOm6bXmXI6mPGMwkqUz96ZlHMKS6kitvX5J3KZL6iMFM\nksrU6GG1fOiMw7lj8VoWrtiYdzmS+oDBTJLK2B+9soVxjbX884+eIKWUdzmSDpHBTJLK2JCaSj7y\nuhk8+NxmbvcRT1LZM5hJUpl7+4nNTB83jCtvf5K9PhhdKmsGM0kqc5UVweXnzGTFhpe44b7n8i5H\n0iFw5n9JGgBeM2Msp0wbxed/+jSnT2+itqp3DzmPgLENtUT07oHqkvqWwUySBoCI4IpzjuL8q37N\n6Z++66D28fYTmvn3d87p28Ik9YrBTJIGiDmTR3DzB07lmfXbe932oec2c9OilZxzzHheO2tcP1Qn\nqScMZpI0gMxrGcW8llG9bvf2E5p5eNVmPvqDxzh52iga6qr7oTpJ3XHwvySJmqoK/uXts1m3bRdX\n3v5k3uVIg5bBTJIEwHGTR3DxK1v473ue80kCUk4MZpKk3/nI66fTPHIIf/vdR9i1d3/e5UiDjsFM\nkvQ7Q2uq+Oe3Hsuy9Tu46udL8y5HGnR6FMwi4uyIWBIRSyPi8g7W10bETdn6eyNiapt1V2TLl0TE\nG7rbZ0R8M1v+WERcGxGOQJWkInr19CbedsIkvnLXMzyxemve5UiDSrfBLCIqgauAc4BZwIURMavd\nZpcAm1JKRwCfBa7M2s4C5gNHA2cDX46Iym72+U1gJnAsMAR4/yEdoSSp1/7vubMYPqSay7/7CPtb\nfTi6VCw9uWI2D1iaUlqWUtoD3Aic326b84Hrs9ffAc6KwvTR5wM3ppR2p5SWA0uz/XW6z5TSj1IG\nuA9oPrRDlCT11sj6Gj523tE8vGoL1/16ed7lSINGT+YxmwSsbPN+FXByZ9uklPZFxBZgdLb8nnZt\nJ2Wvu9xn1oX5XuDPe1CjJKmPvXn2BG558Hn+/X+e4uSW0YxpqOn1PsYMq6W60uHMUk+V8gSzXwbu\nTin9sqOVEXEpcCnAlClTilmXJA0KEcE/veUYXv/Zu3nzl351UPuYOb6B73zoFQyrLeV/bqTS0ZNv\nyvPA5Dbvm7NlHW2zKiKqgOHAhm7adrrPiPgY0AR8oLOiUkpXA1cDzJ071wEQktQPJo4Ywvf+5BU8\n8OymXrfdvHMv/3r7k/z99x/lc+86zgekSz3Qk2C2EDgyIloohKf5wLvbbbMAuAj4LXABcGdKKUXE\nAuBbEfEZYCJwJIVxY9HZPiPi/cAbgLNSSq2HeHySpEM0fVwD08c1HFTbvfta+fc7Cl2h7z7Z3g2p\nO90Gs2zM2GXAT4BK4NqU0uMR8QlgUUppAXAN8I2IWApspBC0yLa7GVgM7AM+nFLaD9DRPrOP/Crw\nLPDb7H9X30spfaLPjliSVDQffs0R3LdiIx//4ePMmTycoycOz7skqaRF4ebH8jZ37ty0aNGivMuQ\nJHVgw/bdnPuFX1FXXcEP//RVPiBdg15E3J9SmtvROm+VkST1q9HDavniu49n5aadXP7dRxkIFwSk\n/mIwkyT1u5OmjuKv3zCD2x5dzTfueTbvcqSSZTCTJBXFpadN48yZY/mnWxfzyKrNeZcjlSSDmSSp\nKCoqgn9/xxzGNtTx4W89wJade/MuSSo5zvgnSSqakfU1fPHdx/POr/6W8770K8Y21PZ6H1NG1fPJ\ntxzDkJrKfqhQypdXzCRJRXXClJF85l3HMWnEEKorK3r1U1kRfO/BVfzpDQ+yb79TXWrg8YqZJKno\nzpszkfPmTDyottf/ZgUfW/A4/7DgcT71lmN8ooAGFIOZJKmsXPSKqazesouv/uIZJg6v47Izj8y7\nJKnPGMwkSWXnb94wg7Vbd/Fv//MU4xrreMfcyd03ksqAwUySVHYqKoIr3z6b9dt2c/n3HqWpoZYz\nZozNuyzpkDn4X5JUlmqqKvjKH5zAjHEN/Mk3H+DRVVvyLkk6ZAYzSVLZaqir5rqLT2Lk0Bou/vp9\nPLfhpbxLkg6JDzGXJJW9peu2c8FXf0MA4xrret2+cUg1n3zLMUwf19D3xUntdPUQc4OZJGlAeHjl\nZq6+exl7D2J+swee28z+1la+ccnJHDNpeD9UJ73MYCZJUhdWvLiD93ztXrbu2svXL57HiYeNzLsk\nDWBdBTPHmEmSBr2pY+q5+YOnMrq+hvdecy+/eebFvEvSIGUwkyQJmDRiCDd/4FSaRw7h4usW8vMl\n6/IuSYOQwUySpMzYxjpuvPRUjhw3jEv/axG3P7Y675I0yDjBrCRJbYyqr+Gb7z+Fi6+7jw9/60Gu\nfPt+XnfUuF7vp7IyGFbrP7PqHf/ESJLUzvAh1XzjkpN5//WL+KtvP3zQ+3nfK6by9+ceRXWlHVTq\nGYOZJEkdqK+t4rqLT+KWh55nx+79vW7/1NptfP03K1iyZhtXvecERtXX9EOVGmgMZpIkdaKuupJ3\nnTTloNufNHUUV3z/Uc770q/4zz+cy1ETGvuwOg1EXluVJKmfvP3EZm7+wKns3d/K2778G378qDcT\nqGsGM0mS+tFxk0fww8texcwJDXzomw/wmf9ZQmtr+U/urv5hV6YkSf1sbGMdN/zxKXz0B4/xhTuX\n8sSabbzt+El5l1WSRtXXMK9lFBGRdym5MJhJklQEddWVfPqC2Rw9sZFP3vYEdyxem3dJJeucY8bz\nqbceOyhvmDCYSZJUJBHBxa9s4Y3HTmDTS3vyLqck/fzJ9XzmjiUsXLGJf73gWM6c2fs55MqZwUyS\npCIb11jHuMa6vMsoSTPHN3LGjCb+4qaH+KOvL+LCeZP56LmzqB8kk/U6+F+SJJWUoyY0cstlr+SD\npx/OjQtXcs7nf8miFRvzLqsoIqXyvzNk7ty5adGiRXmXIUmS+tjCFRv5y5sf4vlNO/mDUw5jyqih\nvd5HbVUF582ZxPCh1f1QYe9FxP0ppbkdrjOYSZKkUrZ99z4+eetibly48qD3MXJoNX/9hpm866TJ\nVFbke8enwUySJJW9nXv2s6+1tdftVrz4Ev9022LuW76Royc28o/nHc3cqaP6ocKeMZhJkqRBLaXE\nDx9ZzT/f9gRrtu7ircdP4vJzZuZyE0ZXwWxw3OIgSZIGtYjgvDkTee1RY/nyz5/h6ruX8ZPH1/An\nZxzOzPG9f4ZpRQXMnTqKxrq+HbfmFTNJkjToPLthB/906xP89ImDn+i3sa6KS141jfe9cirDh/Q8\noNmVKUmS1IFn1m/npd37e91u2669XPebFdyxeG2vA5rBTJIkqR889vwWvvCzp/mfxWtpqKvikle1\ncPErW7oMaI4xkyRJ6gfHTBrO1X8493cB7XM/fZprfrWcI8YOO6j9GcwkSZIO0YGA9vgLW7jml8tZ\nv333Qe3HrkxJkqQi6qor02dlSpIklQiDmSRJUokwmEmSJJUIg5kkSVKJMJhJkiSVCIOZJElSiTCY\nSZIklQiDmSRJUokwmEmSJJUIg5kkSVKJMJhJkiSVCIOZJElSiTCYSZIklYhIKeVdwyGLiG3Akrzr\nKEFjgBfzLqJEeW4657npmOelc56bznluOjbYz8thKaWmjlZUFbuSfrIkpTQ37yJKTUQs8rx0zHPT\nOc9NxzwvnfPcdM5z0zHPS+fsypQkSSoRBjNJkqQSMVCC2dV5F1CiPC+d89x0znPTMc9L5zw3nfPc\ndMzz0okBMfhfkiRpIBgoV8wkSZLKXlkHs4g4OyKWRMTSiLg873ryFBHXRsS6iHiszbJREXFHRDyd\n/ToyzxrzEhGTI+LnEbE4Ih6PiD/Plg/q8xMRdRFxX0Q8nJ2Xf8yWt0TEvdn36qaIqMm71rxERGVE\nPBgRt2bvB/25iYgVEfFoRDwUEYuyZYP6u3RARIyIiO9ExJMR8UREnOq5gYiYkf15OfCzNSL+j+em\nY2UbzCKiErgKOAeYBVwYEbPyrSpXXwfObrfscuBnKaUjgZ9l7wejfcBHUkqzgFOAD2d/Vgb7+dkN\nnJlSmgMcB5wdEacAVwKfTSkdAWwCLsmxxrz9OfBEm/eem4LXpJSOazPdwWD/Lh3weeD2lNJMYA6F\nPzuD/tyklJZkf16OA04EXgK+j+emQ2UbzIB5wNKU0rKU0h7gRuD8nGvKTUrpbmBju8XnA9dnr68H\n3lLUokpESml1SumB7PU2Cn9ZTmKQn59UsD17W539JOBM4DvZ8kF3Xg6IiGbgXOBr2fvAc9OZQf1d\nAoiI4cCrgWsAUkp7Ukqb8dy0dxbwTErpWTw3HSrnYDYJWNnm/apsmV42LqW0Onu9BhiXZzGlICKm\nAscD9+L5OdBV9xCwDrgDeAbYnFLal20ymL9XnwP+BmjN3o/GcwOF8P4/EXF/RFyaLRv03yWgBVgP\nXJd1f38tIurx3LQ3H7ghe+256UA5BzP1Qircfjuob8GNiGHAd4H/k1La2nbdYD0/KaX9WfdCM4Wr\n0DNzLqkkRMSbgHUppfvzrqUEvSqldAKFYSQfjohXt105WL9LFJ6kcwLwlZTS8cAO2nXNDeJzA0A2\nJvM84Nvt1w32c9NWOQez54HJbd43Z8v0srURMQEg+3VdzvXkJiKqKYSyb6aUvpct9vxksi6XnwOn\nAiMi4sDj2gbr9+qVwHkRsYLCMIkzKYwfGvTnJqX0fPbrOgrjhObhdwkKV1BXpZTuzd5/h0JQ89y8\n7BzggZTS2uy956YD5RzMFgJHZndJ1VC4PLog55pKzQLgouz1RcAtOdaSm2xs0DXAEymlz7RZNajP\nT0Q0RcSI7PUQ4HUUxt/9HLgg22zQnReAlNIVKaXmlNJUCn+33JlSeg+D/NxERH1ENBx4DbweeIxB\n/l0CSCmtAVZGxIxs0VnAYjw3bV3Iy92Y4LnpUFlPMBsRb6QwDqQSuDal9KmcS8pNRNwAnAGMAdYC\nHwN+ANwMTAGeBd6ZUmp/g8CAFxGvAn4JPMrL44X+jsI4s0F7fiJiNoUBt5UU/pN2c0rpExExjcJV\nolHAg8AfpJR251dpviLiDOCvUkpvGuznJjv+72dvq4BvpZQ+FRGjGcTfpQMi4jgKN4vUAMuAi8m+\nW3hu6oHngGkppS3ZMv/cdKCsg5kkSdJAUs5dmZIkSQOKwUySJKlEGMwkSZJKhMFMkiSpRBjMJEmS\nSoTBTJIkqUQYzCRJkkqEwUySJKlE/H9QBfoEmr+S2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pEjAnQK9Mhfy"
      },
      "source": [
        "#### **Let's evaluate the best model, on the validation set and compute relevant metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pc4dm0GKcCb7",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "res_cnn.load_weights('base_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_4QyhhkriB7C",
        "outputId": "fa16cdac-04c4-41d4-f0c0-c0c8fed6c0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "## fully balanced training\n",
        "## Rot every 45\n",
        "## 'C1' vs 'C2-3' vs 'all_other' - Downsampling training\n",
        "\n",
        "## 128x128, stride_60,\n",
        "##min_pos_pix_1250, mivalpos_1024\n",
        "## ReduceLROnPlateau(monitor='val_loss'... )\n",
        "## train_batch_32, opt_RMSprop, Kernel_3x3:\n",
        "\n",
        "X, y_true = next(val_generator)\n",
        "y_pred = res_cnn.predict(X)\n",
        "for i in range(1, len(val_generator)):\n",
        "  X, y = next(val_generator)\n",
        "  y_true = np.vstack((y_true, y))\n",
        "  y_pred = np.vstack((y_pred, res_cnn.predict(X)))\n",
        "\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "#roc_auc = roc_auc_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nval_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_acc:\n",
            " 0.6666666666666666\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 9  4  0]\n",
            " [ 4  6  3]\n",
            " [ 0  2 11]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.69      0.69      0.69        13\n",
            "           2       0.50      0.46      0.48        13\n",
            "   all_other       0.79      0.85      0.81        13\n",
            "\n",
            "    accuracy                           0.67        39\n",
            "   macro avg       0.66      0.67      0.66        39\n",
            "weighted avg       0.66      0.67      0.66        39\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}